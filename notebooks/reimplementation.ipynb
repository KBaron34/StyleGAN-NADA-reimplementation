{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "runtime_attributes": {
        "runtime_version": "2025.10"
      }
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "UqJhLckaOvol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Клонируем репозитерий stylegan2-ada-pytorch\n",
        "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git /content/stylegan2-ada-pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alGGyQGjTiEI",
        "outputId": "b5b3f27d-eda4-43ee-9092-ed3f91062efb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path '/content/stylegan2-ada-pytorch' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "s3ZO04O_IwCS"
      },
      "outputs": [],
      "source": [
        "# Скачиваем модель StyleGAN2\n",
        "!mkdir -p /content/StyleGAN2\n",
        "!wget -q https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl -O /content/StyleGAN2/ffhq.pkl # модель обученная на датасете ffhq(лица)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/openai/CLIP.git # установка CLIP\n",
        "!pip install -q ninja # утилита для сборки C++/CUDA кода\n",
        "!pip install -q youtokentome # библиотека от Яндекса для работы с BPE-токенизатором"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqpM4oSTa3zC",
        "outputId": "653cf4a9-ab07-435e-8e69-a89b0257a2af"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for youtokentome (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append('/content/stylegan2-ada-pytorch')\n",
        "os.environ['TORCH_CUDA_ARCH_LIST'] = '7.5'\n",
        "\n",
        "import pickle\n",
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import clip\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision.utils import make_grid, save_image\n",
        "from torchvision.transforms import transforms\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import re\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "RAND = 2"
      ],
      "metadata": {
        "id": "m89WvNfiLX4o"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "f5tk6-ElPt0O"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.manual_seed(RAND)"
      ],
      "metadata": {
        "id": "FItH94jsxcjp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Загружаем свое фото\n",
        "#image =Image.open('/content/kot.jpg')\n",
        "\n",
        "#plt.figure(figsize=(8,8))\n",
        "#plt.imshow(image)\n",
        "#plt.axis('off')\n",
        "#plt.show()"
      ],
      "metadata": {
        "id": "IUtEjDubswvp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "AUNg2UBlz2ZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cleaned_text(text) -> str:\n",
        "    \"\"\"\n",
        "    Простая очистка текста.\n",
        "    \"\"\"\n",
        "    text = str(text) if text is not None else '' # преобразование text в строку, если это не строка\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^а-яёa-z0-9\\s.,*!?:-]', '', text)  # удаление лишних символов (кроме пунктуации)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # удаление лишних пробелов\n",
        "    text = re.sub(r'^[^\\w]+', '', text)  # удаление пунктуации в начале строки\n",
        "    text = text.strip(' .')  # убирает лишние пробелы и точки в начале и в конце строки\n",
        "    return text"
      ],
      "metadata": {
        "id": "1ocd4rypz1el"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing_text(model,\n",
        "                       text: str,\n",
        "                       device: torch.device) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  Делает простую очистку текста, токенизирует и возвращает эмбеддинги.\n",
        "\n",
        "  :param model: модель CLIP\n",
        "  :param text: входной текст\n",
        "  :param device: устройство для вычислений (CPU или CUDA)\n",
        "\n",
        "  :return: эмбеддинг текста в пространстве CLIP\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    text = cleaned_text(text)\n",
        "    text_tokenize = clip.tokenize(text).to(device)\n",
        "    text_features = model.encode_text(text_tokenize)\n",
        "\n",
        "  return text_features.detach()"
      ],
      "metadata": {
        "id": "XU6NTTmIz5XP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing_image(model,\n",
        "                        preprocess_image,\n",
        "                        image,\n",
        "                        device: torch.device) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Если image - torch.Tensor, то преобразует его в PIL.\n",
        "    Если image — PIL.Image, используется стандартный CLIP-препроцессор.\n",
        "    Возвращает эмбеддинг изображения.\n",
        "\n",
        "    :param model: модель CLIP\n",
        "    :param preprocess_image: препроцессор модели CLIP для изображения\n",
        "    :param image: входное изображение\n",
        "    :param device: устройство для вычислений (CPU или CUDA)\n",
        "\n",
        "    :return: эмбеддинг изображения в пространстве CLIP\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    if isinstance(image, torch.Tensor):\n",
        "      # Для torch.Tensor из StyleGAN2 с диапазоном [-1,1]\n",
        "      transform = transforms.Compose([\n",
        "          transforms.Lambda(lambda x: ((x + 1) / 2).clamp(0, 1)),\n",
        "          transforms.Lambda(lambda x: F.interpolate(x,\n",
        "                                                    size=(224, 224),\n",
        "                                                    mode='bilinear',\n",
        "                                                    align_corners=False)),\n",
        "          transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073),\n",
        "                               std=(0.26862954, 0.26130258, 0.27577711))])\n",
        "\n",
        "      image_preproc = transform(image.to(device))\n",
        "\n",
        "      #image = ((image + 1) / 2).clamp(0, 1) # [-1, 1] → [0, 1]\n",
        "      #image = (image * 255).to(torch.uint8) # конвертируем в диапазон [0, 255] и в тип uint8\n",
        "      #image = image.permute(0, 2, 3, 1).cpu().numpy() # меняем формат с CHW → HWC и преобразуем в np.array\n",
        "      #image = [Image.fromarray(img) for img in image] # cоздаём PIL.Image\n",
        "      #image_preproc = torch.stack([preprocess(img) for img in image]).to(device)\n",
        "    else:\n",
        "      # Для PIL-изображений\n",
        "      image_preproc = preprocess_image(image).unsqueeze(0).to(device)\n",
        "\n",
        "    image_features = model.encode_image(image_preproc)\n",
        "\n",
        "    return image_features"
      ],
      "metadata": {
        "id": "soh9mInTz8OD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_latents(G,\n",
        "                 RAND: int,\n",
        "                 device: torch.device):\n",
        "  \"\"\"\n",
        "  Герерирует латенты.\n",
        "\n",
        "  :param G: генератор, используется для определение размера вектора\n",
        "  :param RAND: фиксирует генератор случайных чисел\n",
        "  :param device: устройство для вычислений (CPU или CUDA)\n",
        "\n",
        "  :return: два латента (latent, latent_sample)\n",
        "  \"\"\"\n",
        "  torch.cuda.manual_seed(RAND)\n",
        "  latent_sample = torch.randn(4, G.z_dim, device=device)\n",
        "  latent = torch.randn(2, G.z_dim, device=device)\n",
        "  latent_plus = torch.randn(8, G.z_dim, device=device)\n",
        "  return latent_sample, latent, latent_plus"
      ],
      "metadata": {
        "id": "XRe-QXGHstwv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# StyleGN2-ADA"
      ],
      "metadata": {
        "id": "28rq0iJCfTpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/StyleGAN2/ffhq.pkl', 'rb') as f:\n",
        "  G = pickle.load(f)['G_ema'].to(device)\n",
        "\n",
        "G_frozen = copy.deepcopy(G) # копия генератора (замороженный)"
      ],
      "metadata": {
        "id": "3w0AIJzkO0oC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_sample, latent, latent_plus = make_latents(G,\n",
        "                                                  RAND=RAND,\n",
        "                                                  device=device)\n",
        "\n",
        "c = None\n",
        "G.eval()\n",
        "image_gan = G(latent_sample,\n",
        "              c,\n",
        "              truncation_psi=0.7,\n",
        "              noise_mode='const')"
      ],
      "metadata": {
        "id": "14L1HxUCQP4q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68a2b5c2-43ad-4083-d8f0-214f04478ea7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img = ((image_gan +  1) / 2).clamp(0, 1)  # [-1, 1] → [0, 1]\n",
        "grid = make_grid(img, nrow=2)\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(grid.permute(1, 2, 0).detach().cpu().numpy())\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p_s6V43nWngb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLIP"
      ],
      "metadata": {
        "id": "9j_129aafZyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Загружаем модель и процессор\n",
        "model, preprocess = clip.load('ViT-B/32', device=device)"
      ],
      "metadata": {
        "id": "-xeJbc1cvWRn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_features = preprocessing_text(model, 'woman', device)"
      ],
      "metadata": {
        "id": "H8i9_wYiHZ9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_features = preprocessing_image(model, preprocess, image_gan, device)"
      ],
      "metadata": {
        "id": "XTfSu2fnHZ5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "wkUFqL6Uy5C2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss"
      ],
      "metadata": {
        "id": "wtECd3_AzxxB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### cosine_dist"
      ],
      "metadata": {
        "id": "OpuwYAT1l1i_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_dist(image_features: torch.Tensor,\n",
        "                text_features: torch.Tensor) -> torch.Tensor:\n",
        " \"\"\"\n",
        " Вычисляет глобальный CLIP-loss (cosine distance).\n",
        "\n",
        " :param image_features: вектора изображения в пространстве CLIP\n",
        " :param text_features: вектора текста в пространстве CLIP\n",
        "\n",
        " :return: косинусное расстояние\n",
        " \"\"\"\n",
        " global_loss = 1 - F.cosine_similarity(image_features, text_features, dim=1)\n",
        " return global_loss.mean()"
      ],
      "metadata": {
        "id": "YNVpbgAHzxXh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### clip_loss"
      ],
      "metadata": {
        "id": "V8o8irlDl32d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def delta_text(model,\n",
        "               text_target: str,\n",
        "               text_source: str,\n",
        "               device: torch.device) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  Вычисляет вектор текстового направления между таргетом и текущим стилем.\n",
        "\n",
        "  :param model: модель CLIP\n",
        "  :param text_target: текст целевого стиля\n",
        "  :param text_source: текст исходного стиля\n",
        "  :param device: устройство для вычислений (CPU или CUDA)\n",
        "\n",
        "  :return: вектор текстового направления\n",
        "  \"\"\"\n",
        "  emb_delta_text = preprocessing_text(model,\n",
        "                                      text_target,\n",
        "                                      device) - preprocessing_text(model,\n",
        "                                                                   text_source,\n",
        "                                                                   device)\n",
        "  return emb_delta_text"
      ],
      "metadata": {
        "id": "xNBJyxzCAT3a"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def delta_image(model,\n",
        "                preprocess_image,\n",
        "                image: torch.Tensor,\n",
        "                image_frozen: torch.Tensor,\n",
        "                device: torch.device) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  Вычисляет вектор визуального направления между изображением до и после fine-tuning.\n",
        "  :param model: модель CLIP\n",
        "  :param preprocess_image: препроцессор модели CLIP для изображения\n",
        "  :param image: изображение из обучаемого генератора\n",
        "  :param image_frozen: изображение из замороженного генератора\n",
        "  :param device: устройство для вычислений (CPU или CUDA)\n",
        "\n",
        "  :return: вектор визуального направления\n",
        "  \"\"\"\n",
        "\n",
        "  emb_delta_image = preprocessing_image(model,\n",
        "                                        preprocess_image,\n",
        "                                        image,\n",
        "                                        device) - preprocessing_image(model,\n",
        "                                                                      preprocess_image,\n",
        "                                                                      image_frozen,\n",
        "                                                                      device)\n",
        "  return emb_delta_image"
      ],
      "metadata": {
        "id": "6DKjMJjt1Ejw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clip_loss(delta_image: torch.Tensor,\n",
        "              delta_text: torch.Tensor) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  Вычисляет направленный CLIP-loss.\n",
        "\n",
        "  :param delta_image: вектор визуального направления\n",
        "  :param delta_text: вектор текстового направления\n",
        "\n",
        "  :return: направленный CLIP-loss (скаляр)\n",
        "  \"\"\"\n",
        "\n",
        "  direction_clip_loss = 1 - F.cosine_similarity(delta_image, delta_text, dim=1)\n",
        "  return direction_clip_loss.mean()"
      ],
      "metadata": {
        "id": "CSrnhQ_8DbtQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "uRpZMN5H0SKt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### cosine_dist"
      ],
      "metadata": {
        "id": "bpmEQjMQVUfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_generator(generator,\n",
        "                    model,\n",
        "                    preprocess_image,\n",
        "                    text: str,\n",
        "                    epochs: int,\n",
        "                    criterion,\n",
        "                    device: torch.device,\n",
        "                    RAND: int=2,\n",
        "                    batch_size: int=2,\n",
        "                    sample_size: int=4,\n",
        "                    lr: float=0.0003,\n",
        "                    c: torch.Tensor=None):\n",
        "  \"\"\"\n",
        "  Обучает генератор StyleGAN2 для изменения изображений в направлении текста\n",
        "  с использованием косинусного расстояния в качестве функции потерь.\n",
        "\n",
        "  Функция выполняет fine-tuning генератора на фиксированном латентном векторе,\n",
        "  вычисляет эмбеддинги изображения и текста через CLIP и минимизирует\n",
        "  косинусное расстояние между ними, чтобы стилизовать изображение под заданный текст.\n",
        "\n",
        "  Каждые 50 эпох сохраняет сгенерированные изображения.\n",
        "  В конце визуализирует поведение функции потерь.\n",
        "\n",
        "  :param generator: генератор StyleGAN2, который будет обучаться\n",
        "  :param model: модель CLIP для получения текстовых и визуальных эмбеддингов\n",
        "  :param preprocess_image: функция препроцессинга изображений для CLIP\n",
        "  :param text: целевой текстовый промпт, определяющий желаемый стиль изображения\n",
        "  :param epochs: количество эпох обучения генератора\n",
        "  :param criterion: функция потерь, которая принимает текстовые и визуальные эмбеддинги и возвращает скалярный loss\n",
        "  :param device: устройство для вычислений (CPU или CUDA)\n",
        "  :param RAND: фиксирует генератор случайных чисел\n",
        "  :param batch_size: размерность батча для обучения\n",
        "  :param sample_size: размерность генерируемого сэмпла\n",
        "  :param lr: скорость обучения для оптимизатора\n",
        "  :param c: условие для генератора\n",
        "\n",
        "  :return: None\n",
        "  \"\"\"\n",
        "  loss_lst = []\n",
        "\n",
        "  torch.cuda.manual_seed(RAND)\n",
        "  os.makedirs('results', exist_ok=True)\n",
        "\n",
        "  latent_sample = torch.randn(sample_size, generator.z_dim, device=device)\n",
        "  latent = torch.randn(batch_size, generator.z_dim, device=device)\n",
        "\n",
        "  optimizer = torch.optim.AdamW(generator.parameters(), lr=lr)\n",
        "\n",
        "  generator.train()\n",
        "  for param in generator.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "  for i in tqdm(range(epochs)):\n",
        "    torch.cuda.empty_cache()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    image = generator(latent, c, truncation_psi=0.7, noise_mode='const')\n",
        "\n",
        "    text_features = preprocessing_text(model,\n",
        "                                       text,\n",
        "                                       device)\n",
        "    image_features = preprocessing_image(model,\n",
        "                                         preprocess,\n",
        "                                         image,\n",
        "                                         device)\n",
        "\n",
        "    loss = criterion(text_features, image_features)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_lst.append(loss.item())\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    print(f'Эпоха {i + 1}: loss={loss.item():.4f}')\n",
        "\n",
        "    if i % 50 == 0:\n",
        "      image_sample = generator(latent_sample,\n",
        "                               c,\n",
        "                               truncation_psi=0.7,\n",
        "                               noise_mode='const')\n",
        "      img = (image_sample.clamp(-1, 1) + 1) / 2.0  # [-1, 1] → [0, 1]\n",
        "      grid = make_grid(img, nrow=2)\n",
        "\n",
        "      save_path = f'results/generated_image_epoch_{i}.jpg'\n",
        "      save_image(grid, save_path, 'jpeg')\n",
        "\n",
        "      plt.figure(figsize=(8, 8))\n",
        "      plt.imshow(grid.permute(1, 2, 0).detach().cpu().numpy())\n",
        "      plt.axis('off')\n",
        "      plt.show()\n",
        "\n",
        "  plt.figure(figsize=(8, 8))\n",
        "  plt.plot(loss_lst)\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel('loss')\n",
        "  plt.title('График лосса')\n",
        "  plt.grid(True)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "ZI9o7uPqy7dC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator(generator=G,\n",
        "                model=model,\n",
        "                preprocess_image=preprocess,\n",
        "                text='werewolf',\n",
        "                epochs=301,\n",
        "                criterion=cosine_dist,\n",
        "                device=device,\n",
        "                RAND=2,\n",
        "                batch_size=2,\n",
        "                sample_size=4,\n",
        "                lr=0.0003,\n",
        "                c=None)"
      ],
      "metadata": {
        "id": "zFhHsdI3DqQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### clip_loss"
      ],
      "metadata": {
        "id": "B8d1K1aQVcWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_generator(generator,\n",
        "                    generator_frozen,\n",
        "                    model,\n",
        "                    preprocess_image,\n",
        "                    text_source: str,\n",
        "                    text_target: str,\n",
        "                    epochs: int,\n",
        "                    criterion,\n",
        "                    device: torch.device,\n",
        "                    RAND: int=2,\n",
        "                    batch_size: int=2,\n",
        "                    sample_size: int=4,\n",
        "                    batch_size_w: int=8,\n",
        "                    lr: float=0.002,\n",
        "                    k: int=12,\n",
        "                    c: torch.Tensor=None):\n",
        "  \"\"\"\n",
        "  Обучает генератор StyleGAN2 для изменения изображений в направлении текста\n",
        "  с использованием CLIP-косинусного расстояния в качестве функции потерь.\n",
        "\n",
        "  Функция выполняет fine-tuning генератора на фиксированном латентном векторе,\n",
        "  вычисляет векторное направление между изображениями до и после обучения\n",
        "  и текстовыми эмбеддингами, и минимизирует косинусное расстояние между ними.\n",
        "\n",
        "  Каждые 50 эпох сохраняет сгенерированные изображения.\n",
        "  В конце визуализирует график лосса.\n",
        "\n",
        "  :param generator: генератор StyleGAN2, который будет обучаться\n",
        "  :param generator_frozen: замороженный генератор, используемый для вычисления delta_image\n",
        "  :param model: модель CLIP для получения текстовых и визуальных эмбеддингов\n",
        "  :param preprocess_image: функция препроцессинга изображений для CLIP\n",
        "  :param text_source: исходный текстовый промпт\n",
        "  :param text_target: целевой текстовый промпт\n",
        "  :param epochs: количество эпох обучения генератора\n",
        "  :param criterion: функция потерь, которая принимает delta_image и delta_text и возвращает скалярный loss\n",
        "  :param device: устройство для вычислений (CPU или CUDA)\n",
        "  :param RAND: фиксирует генератор случайных чисел\n",
        "  :param batch_size: размерность батча для обучения\n",
        "  :param sample_size: размерность генерируемого сэмпла\n",
        "  :param batch_size_w: размерность батча для оптимизации векторов из пространства W+\n",
        "  :param lr: скорость обучения для оптимизатора\n",
        "  :param k: количество слоев которые обучаются\n",
        "  :param c: условие для генератора\n",
        "\n",
        "  :return: None\n",
        "  \"\"\"\n",
        "  loss_lst = []\n",
        "\n",
        "  torch.cuda.manual_seed(RAND)\n",
        "  os.makedirs('results', exist_ok=True)\n",
        "\n",
        "  latent_sample = torch.randn(sample_size, generator.z_dim, device=device)\n",
        "  latent = torch.randn(batch_size, generator.z_dim, device=device)\n",
        "  latent_plus = torch.randn(batch_size_w, generator.z_dim, device=device)\n",
        "\n",
        "  generator = generator.to(device)\n",
        "  generator_frozen = generator_frozen.to(device)\n",
        "  generator.train()\n",
        "  generator_frozen.eval()\n",
        "\n",
        "  lst_block = []\n",
        "  for name, module in generator.synthesis.named_children():\n",
        "    if name.startswith('b'):\n",
        "        lst_block.append(name)\n",
        "\n",
        "  for param in generator_frozen.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "\n",
        "    # Оптимизация векторов из пространства W+\n",
        "    w_plus_source = generator.mapping(latent_plus,\n",
        "                                      c) # [batch_size, num_layers, dim]\n",
        "    w_plus_target = generator.mapping(latent_plus,\n",
        "                                      c).requires_grad_(True) # [batch_size, num_layers, dim]\n",
        "\n",
        "    optimizer_w_plus = torch.optim.Adam([w_plus_target], lr=lr)\n",
        "\n",
        "    optimizer_w_plus.zero_grad()\n",
        "\n",
        "    image_w_plus = generator.synthesis(w_plus_target,\n",
        "                                       noise_mode='const')\n",
        "    image_w_plus_features = preprocessing_image(model,\n",
        "                                                preprocess,\n",
        "                                                image_w_plus,\n",
        "                                                device)\n",
        "\n",
        "    text_w_plus_features = preprocessing_text(model,\n",
        "                                              text_target,\n",
        "                                              device)\n",
        "\n",
        "    loss_w_plus = cosine_dist(image_w_plus_features,\n",
        "                              text_w_plus_features)\n",
        "\n",
        "    loss_w_plus.backward()\n",
        "    optimizer_w_plus.step()\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    delta_w_plus = w_plus_target.detach() - w_plus_source\n",
        "    delta_norm = delta_w_plus.norm(p=2, dim=-1).mean(dim=0)\n",
        "\n",
        "    topk_indices = torch.topk(delta_norm, k).indices\n",
        "\n",
        "    # Заморозка mapping\n",
        "    for param in generator.mapping.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "    # Заморозка synthesis\n",
        "    for p in generator.synthesis.parameters():\n",
        "      p.requires_grad = False\n",
        "\n",
        "    # Заморозка affine и torgb параметров\n",
        "    #for name, module in generator.synthesis.named_modules():\n",
        "    #  lname = name.lower()\n",
        "    #  if 'affine' in lname or 'torgb' in lname:\n",
        "    #    for p in module.parameters():\n",
        "    #      p.requires_grad = False\n",
        "\n",
        "    # Заморозка всех слоев кроме топ-k\n",
        "    for ind in topk_indices:\n",
        "      block_idx = ind // 2\n",
        "      conv_num = ind % 2\n",
        "\n",
        "      block_name = lst_block[block_idx]\n",
        "      block = getattr(generator.synthesis, block_name)\n",
        "\n",
        "      if block_name == 'b4':\n",
        "        block.conv1.weight.requires_grad = True\n",
        "        block.conv1.bias.requires_grad = True\n",
        "        #for p in conv.parameters():\n",
        "        #    p.requires_grad = True\n",
        "      else:\n",
        "        if conv_num == 0:\n",
        "          block.conv0.weight.requires_grad = True\n",
        "          block.conv0.bias.requires_grad = True\n",
        "        else:\n",
        "          block.conv1.weight.requires_grad = True\n",
        "          block.conv1.bias.requires_grad = True\n",
        "        #for p in conv.parameters():\n",
        "        #  p.requires_grad = True\n",
        "\n",
        "    train_params = [param for param in generator.parameters() if param.requires_grad]\n",
        "    optimizer = torch.optim.Adam(train_params, lr=lr)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    image = generator(latent,\n",
        "                      c,\n",
        "                      truncation_psi=0.7,\n",
        "                      noise_mode='const')\n",
        "    image_frozen = generator_frozen(latent,\n",
        "                                    c,\n",
        "                                    truncation_psi=0.7,\n",
        "                                    noise_mode='const')\n",
        "\n",
        "    emb_delta_image = delta_image(model,\n",
        "                                  preprocess_image,\n",
        "                                  image,\n",
        "                                  image_frozen,\n",
        "                                  device)\n",
        "\n",
        "    emb_delta_text = delta_text(model,\n",
        "                                text_target,\n",
        "                                text_source,\n",
        "                                device)\n",
        "\n",
        "    loss = criterion(emb_delta_image,\n",
        "                     emb_delta_text) * 50\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_lst.append(loss.item())\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    print(f'Эпоха {epoch + 1}: loss={loss.item():.4f}')\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "      image_sample = generator(latent_sample,\n",
        "                               c,\n",
        "                               truncation_psi=0.7,\n",
        "                               noise_mode='const')\n",
        "      img = (image_sample.clamp(-1, 1) + 1) / 2.0  # [-1, 1] → [0, 1]\n",
        "      grid = make_grid(img, nrow=2)\n",
        "\n",
        "      save_path = f'results/generated_image_epoch_{epoch}.jpg'\n",
        "      save_image(grid, save_path, 'jpeg')\n",
        "\n",
        "      plt.figure(figsize=(8, 8))\n",
        "      plt.imshow(grid.permute(1, 2, 0).detach().cpu().numpy())\n",
        "      plt.axis('off')\n",
        "      plt.show()\n",
        "\n",
        "  plt.figure(figsize=(8, 8))\n",
        "  plt.plot(loss_lst)\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel('loss')\n",
        "  plt.title('График лосса')\n",
        "  plt.grid(True)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "xNZlBxSMVO1x"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator(generator=G,\n",
        "                generator_frozen=G_frozen,\n",
        "                model=model,\n",
        "                preprocess_image=preprocess,\n",
        "                text_source='human',\n",
        "                text_target='werewolf',\n",
        "                epochs=301,\n",
        "                criterion=clip_loss,\n",
        "                device=device,\n",
        "                lr=0.002,\n",
        "                c=None)"
      ],
      "metadata": {
        "id": "gW9u3Sswnzwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y6XIPTepIa8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LcD9ciHXqhzX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}