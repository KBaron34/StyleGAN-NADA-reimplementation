{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "runtime_attributes": {
        "runtime_version": "2025.10"
      }
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "UqJhLckaOvol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Клонируем репозитерий stylegan2-ada-pytorch\n",
        "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git /content/stylegan2-ada-pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alGGyQGjTiEI",
        "outputId": "65457b89-6ce8-45d5-ec75-9cf570902dd1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '/content/stylegan2-ada-pytorch'...\n",
            "remote: Enumerating objects: 131, done.\u001b[K\n",
            "remote: Counting objects: 100% (2/2), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 131 (delta 0), reused 0 (delta 0), pack-reused 129 (from 2)\u001b[K\n",
            "Receiving objects: 100% (131/131), 1.13 MiB | 37.35 MiB/s, done.\n",
            "Resolving deltas: 100% (57/57), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3ZO04O_IwCS"
      },
      "outputs": [],
      "source": [
        "# Скачиваем модель StyleGAN2\n",
        "!mkdir -p /content/StyleGAN2\n",
        "!wget -q https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl -O /content/StyleGAN2/ffhq.pkl # модель обученная на датасете ffhq(лица)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/openai/CLIP.git # установка CLIP\n",
        "!pip install -q ninja # утилита для сборки C++/CUDA кода\n",
        "!pip install -q youtokentome # библиотека от Яндекса для работы с BPE-токенизатором"
      ],
      "metadata": {
        "id": "oqpM4oSTa3zC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append('/content/stylegan2-ada-pytorch')\n",
        "os.environ['TORCH_CUDA_ARCH_LIST'] = '7.5'\n",
        "\n",
        "import pickle\n",
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import clip\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "from torchvision.utils import make_grid, save_image\n",
        "from torchvision.transforms import transforms\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import re\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "RAND = 2"
      ],
      "metadata": {
        "id": "m89WvNfiLX4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "f5tk6-ElPt0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Загружаем свое фото\n",
        "#image =Image.open('/content/kot.jpg')\n",
        "\n",
        "#plt.figure(figsize=(8,8))\n",
        "#plt.imshow(image)\n",
        "#plt.axis('off')\n",
        "#plt.show()"
      ],
      "metadata": {
        "id": "IUtEjDubswvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "AUNg2UBlz2ZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEMPLATES = [\n",
        "    'a bad photo of a {}.',\n",
        "    'a sculpture of a {}.',\n",
        "    'a photo of the hard to see {}.',\n",
        "    'a low resolution photo of the {}.',\n",
        "    'a rendering of a {}.',\n",
        "    'graffiti of a {}.',\n",
        "    'a bad photo of the {}.',\n",
        "    'a cropped photo of the {}.',\n",
        "    'a tattoo of a {}.',\n",
        "    'the embroidered {}.',\n",
        "    'a photo of a hard to see {}.',\n",
        "    'a bright photo of a {}.',\n",
        "    'a photo of a clean {}.',\n",
        "    'a photo of a dirty {}.',\n",
        "    'a dark photo of the {}.',\n",
        "    'a drawing of a {}.',\n",
        "    'a photo of my {}.',\n",
        "    'the plastic {}.',\n",
        "    'a photo of the cool {}.',\n",
        "    'a close-up photo of a {}.',\n",
        "    'a black and white photo of the {}.',\n",
        "    'a painting of the {}.',\n",
        "    'a painting of a {}.',\n",
        "    'a pixelated photo of the {}.',\n",
        "    'a sculpture of the {}.',\n",
        "    'a bright photo of the {}.',\n",
        "    'a cropped photo of a {}.',\n",
        "    'a plastic {}.',\n",
        "    'a photo of the dirty {}.',\n",
        "    'a jpeg corrupted photo of a {}.',\n",
        "    'a blurry photo of the {}.',\n",
        "    'a photo of the {}.',\n",
        "    'a good photo of the {}.',\n",
        "    'a rendering of the {}.',\n",
        "    'a {} in a video game.',\n",
        "    'a photo of one {}.',\n",
        "    'a doodle of a {}.',\n",
        "    'a close-up photo of the {}.',\n",
        "    'a photo of a {}.',\n",
        "    'the origami {}.',\n",
        "    'the {} in a video game.',\n",
        "    'a sketch of a {}.',\n",
        "    'a doodle of the {}.',\n",
        "    'a origami {}.',\n",
        "    'a low resolution photo of a {}.',\n",
        "    'the toy {}.',\n",
        "    'a rendition of the {}.',\n",
        "    'a photo of the clean {}.',\n",
        "    'a photo of a large {}.',\n",
        "    'a rendition of a {}.',\n",
        "    'a photo of a nice {}.',\n",
        "    'a photo of a weird {}.',\n",
        "    'a blurry photo of a {}.',\n",
        "    'a cartoon {}.',\n",
        "    'art of a {}.',\n",
        "    'a sketch of the {}.',\n",
        "    'a embroidered {}.',\n",
        "    'a pixelated photo of a {}.',\n",
        "    'itap of the {}.',\n",
        "    'a jpeg corrupted photo of the {}.',\n",
        "    'a good photo of a {}.',\n",
        "    'a plushie {}.',\n",
        "    'a photo of the nice {}.',\n",
        "    'a photo of the small {}.',\n",
        "    'a photo of the weird {}.',\n",
        "    'the cartoon {}.',\n",
        "    'art of the {}.',\n",
        "    'a drawing of the {}.',\n",
        "    'a photo of the large {}.',\n",
        "    'a black and white photo of a {}.',\n",
        "    'the plushie {}.',\n",
        "    'a dark photo of a {}.',\n",
        "    'itap of a {}.',\n",
        "    'graffiti of the {}.',\n",
        "    'a toy {}.',\n",
        "    'itap of my {}.',\n",
        "    'a photo of a cool {}.',\n",
        "    'a photo of a small {}.',\n",
        "    'a tattoo of the {}.'\n",
        "]"
      ],
      "metadata": {
        "id": "k_7Wm47yY-wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleaned_text(text) -> str:\n",
        "    \"\"\"\n",
        "    Простая очистка текста.\n",
        "    \"\"\"\n",
        "    text = str(text) if text is not None else '' # преобразование text в строку, если это не строка\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^а-яёa-z0-9\\s.,*!?:-]', '', text)  # удаление лишних символов (кроме пунктуации)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # удаление лишних пробелов\n",
        "    text = re.sub(r'^[^\\w]+', '', text)  # удаление пунктуации в начале строки\n",
        "    text = text.strip(' .')  # убирает лишние пробелы и точки в начале и в конце строки\n",
        "    return text"
      ],
      "metadata": {
        "id": "1ocd4rypz1el"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prompt_list(word: str):\n",
        "  \"\"\"\n",
        "  Создает список промптов.\n",
        "  \"\"\"\n",
        "  word = cleaned_text(word)\n",
        "  return [t.format(word) for t in TEMPLATES]"
      ],
      "metadata": {
        "id": "2tktiLgJbL6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_clip_models(model_names: list[str],\n",
        "                     device: torch.device,\n",
        "                     weights: list[float]=[1.0, 1.0]) -> list:\n",
        "  \"\"\"\n",
        "  Загружает модели и препроцессоры CLIP.\n",
        "\n",
        "  :param model_names: список имен моделей CLIP\n",
        "  :param device: устройство для вычислений (CPU или CUDA)\n",
        "  :param weights: вклад модели CLIP в итоговый эмбеддинг\n",
        "\n",
        "  :return: список моделей, препроцессоров и весов\n",
        "  \"\"\"\n",
        "  models = []\n",
        "\n",
        "  for name, w in zip(model_names, weights):\n",
        "    model, preprocess = clip.load(name, device=device)\n",
        "    for p in model.parameters():\n",
        "      p.requires_grad = False\n",
        "    models.append((model, preprocess, w))\n",
        "\n",
        "  return models"
      ],
      "metadata": {
        "id": "cIHMIz7VVZSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing_text(models: list,\n",
        "                       prompts: list[str],\n",
        "                       device: torch.device) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  Принимает список текстовых промптов,\n",
        "  возвращает один усреднённый и нормализованный эмбеддинг.\n",
        "\n",
        "  :param models: список моделей, препроцессоров и весов CLIP\n",
        "  :param prompts: список текстовых промптов\n",
        "  :param device: устройство для вычислений (CPU или CUDA)\n",
        "\n",
        "  :return: эмбеддинг текста в пространстве CLIP\n",
        "  \"\"\"\n",
        "  total_embedding = 0.0\n",
        "  weights_sum = 0.0\n",
        "\n",
        "  for model in models:\n",
        "    weights_sum += model[2]\n",
        "\n",
        "  tokens = clip.tokenize(prompts).to(device)\n",
        "\n",
        "  for model in models:\n",
        "    model[0].eval()\n",
        "    text_features = model[0].encode_text(tokens)\n",
        "    # Нормализация\n",
        "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "    # Усреднение всех промптов\n",
        "    text_mean = text_features.mean(dim=0, keepdim=True)\n",
        "    # Нормализация\n",
        "    text_mean = text_mean / text_mean.norm(dim=-1, keepdim=True)\n",
        "    # Эмбеддинг с учетом веcа модели\n",
        "    total_embedding += text_mean * (model[2] / weights_sum)\n",
        "\n",
        "  # Нормализация\n",
        "  total_embedding = total_embedding / total_embedding.norm(dim=-1, keepdim=True)\n",
        "\n",
        "  return total_embedding.detach()"
      ],
      "metadata": {
        "id": "XU6NTTmIz5XP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing_image(models: list,\n",
        "                        image,\n",
        "                        device: torch.device) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  Если image - torch.Tensor, то преобразует его в PIL.\n",
        "  Если image — PIL.Image, используется стандартный CLIP-препроцессор.\n",
        "  Возвращает нормализованный эмбеддинг изображения.\n",
        "\n",
        "  :param models: список моделей, препроцессоров и весов CLIP\n",
        "  :param image: входное изображение\n",
        "  :param device: устройство для вычислений (CPU или CUDA)\n",
        "\n",
        "  :return: эмбеддинг изображения в пространстве CLIP\n",
        "  \"\"\"\n",
        "\n",
        "  total_embedding = 0.0\n",
        "  weights_sum = 0.0\n",
        "\n",
        "  for model in models:\n",
        "    weights_sum += model[2]\n",
        "\n",
        "  if isinstance(image, torch.Tensor):\n",
        "    # Для torch.Tensor из StyleGAN2 с диапазоном [-1,1]\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Lambda(lambda x: ((x + 1) / 2).clamp(0, 1)),\n",
        "        transforms.Lambda(lambda x: F.interpolate(x,\n",
        "                                                  size=(224, 224),\n",
        "                                                  mode='bicubic',\n",
        "                                                  align_corners=False)),\n",
        "        transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073),\n",
        "                             std=(0.26862954, 0.26130258, 0.27577711))])\n",
        "\n",
        "    image_preproc = transform(image.to(device))\n",
        "\n",
        "    for model in models:\n",
        "      model[0].eval()\n",
        "      image_features = model[0].encode_image(image_preproc)\n",
        "      # Нормализация\n",
        "      image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "      # Эмбеддинг с учетом веcа модели\n",
        "      total_embedding += image_features * (model[2] / weights_sum)\n",
        "\n",
        "    # Нормализация\n",
        "    total_embedding = total_embedding / total_embedding.norm(dim=-1, keepdim=True)\n",
        "\n",
        "  else:\n",
        "    # Для PIL-изображений\n",
        "    for model in models:\n",
        "      model.eval()\n",
        "      image_preproc = model[1](image).unsqueeze(0).to(device)\n",
        "      image_features = model[0].encode_image(image_preproc)\n",
        "\n",
        "      # Нормализация\n",
        "      image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "      # Эмбеддинг с учетом веcа модели\n",
        "      total_embedding += image_features * (model[2] / weights_sum)\n",
        "\n",
        "    # Нормализация\n",
        "    total_embedding = total_embedding / total_embedding.norm(dim=-1, keepdim=True)\n",
        "\n",
        "  return total_embedding"
      ],
      "metadata": {
        "id": "soh9mInTz8OD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_patches(image: torch.Tensor,\n",
        "                    patch_size: int=64,\n",
        "                    num_patches: int=8):\n",
        "    \"\"\"\n",
        "    Извлекает случайные патчи из изображения.\n",
        "\n",
        "    :param image: тензор [B, C, H, W]\n",
        "    :param patch_size: размер квадрата патча\n",
        "    :param num_patches: количество патчей\n",
        "\n",
        "    :return: тензор [B*num_patches, C, patch_size, patch_size]\n",
        "    \"\"\"\n",
        "    B, C, H, W = image.shape\n",
        "    patches = []\n",
        "    for _ in range(num_patches):\n",
        "        i = torch.randint(0, H - patch_size, (1,)).item()\n",
        "        j = torch.randint(0, W - patch_size, (1,)).item()\n",
        "        patch = image[:, :, i:i+patch_size, j:j+patch_size]\n",
        "\n",
        "        patches.append(patch)\n",
        "\n",
        "    return torch.cat(patches, dim=0)"
      ],
      "metadata": {
        "id": "SddG65QzFqsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_latents(G,\n",
        "                 RAND: int,\n",
        "                 device: torch.device):\n",
        "  \"\"\"\n",
        "  Герерирует латенты.\n",
        "\n",
        "  :param G: генератор, используется для определение размера вектора\n",
        "  :param RAND: фиксирует генератор случайных чисел\n",
        "  :param device: устройство для вычислений (CPU или CUDA)\n",
        "\n",
        "  :return: два латента (latent, latent_sample)\n",
        "  \"\"\"\n",
        "  torch.cuda.manual_seed(RAND)\n",
        "  latent_sample = torch.randn(4, G.z_dim, device=device)\n",
        "  latent = torch.randn(2, G.z_dim, device=device)\n",
        "  latent_plus = torch.randn(8, G.z_dim, device=device)\n",
        "\n",
        "  return latent_sample, latent, latent_plus"
      ],
      "metadata": {
        "id": "XRe-QXGHstwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# StyleGN2-ADA"
      ],
      "metadata": {
        "id": "28rq0iJCfTpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/StyleGAN2/ffhq.pkl', 'rb') as f:\n",
        "  G = pickle.load(f)['G_ema'].to(device)\n",
        "\n",
        "G_frozen = copy.deepcopy(G) # копия генератора (замороженный)"
      ],
      "metadata": {
        "id": "3w0AIJzkO0oC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_sample, latent, latent_plus = make_latents(G,\n",
        "                                                  RAND=RAND,\n",
        "                                                  device=device)\n",
        "\n",
        "c = None\n",
        "G.eval()\n",
        "image_gan = G(latent_sample,\n",
        "              c,\n",
        "              truncation_psi=0.7,\n",
        "              noise_mode='const')"
      ],
      "metadata": {
        "id": "14L1HxUCQP4q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2f16543-1ce1-4e7a-bd41-99d152b668f4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img = ((image_gan +  1) / 2).clamp(0, 1)  # [-1, 1] → [0, 1]\n",
        "grid = make_grid(img, nrow=2)\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(grid.permute(1, 2, 0).detach().cpu().numpy())\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p_s6V43nWngb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLIP"
      ],
      "metadata": {
        "id": "9j_129aafZyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Загружаем модель и процессор\n",
        "model, preprocess = clip.load('ViT-B/32', device=device)"
      ],
      "metadata": {
        "id": "-xeJbc1cvWRn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "954ab98e-ac16-48d8-951e-57a070c4e2a7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 338M/338M [00:03<00:00, 97.4MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_features = preprocessing_text(model, 'woman', device)"
      ],
      "metadata": {
        "id": "H8i9_wYiHZ9v"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_features = preprocessing_image(model, preprocess, image_gan, device)"
      ],
      "metadata": {
        "id": "XTfSu2fnHZ5m"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "wkUFqL6Uy5C2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss"
      ],
      "metadata": {
        "id": "wtECd3_AzxxB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### cosine_dist"
      ],
      "metadata": {
        "id": "OpuwYAT1l1i_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_dist(image_features: torch.Tensor,\n",
        "                text_features: torch.Tensor) -> torch.Tensor:\n",
        " \"\"\"\n",
        " Вычисляет глобальный CLIP-loss (cosine distance).\n",
        "\n",
        " :param image_features: вектора изображения в пространстве CLIP\n",
        " :param text_features: вектора текста в пространстве CLIP\n",
        "\n",
        " :return: косинусное расстояние\n",
        " \"\"\"\n",
        " global_loss = 1 - F.cosine_similarity(image_features, text_features, dim=-1)\n",
        "\n",
        " return global_loss.mean()"
      ],
      "metadata": {
        "id": "YNVpbgAHzxXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### clip_loss"
      ],
      "metadata": {
        "id": "V8o8irlDl32d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def delta_text(models: list,\n",
        "               text_target: str,\n",
        "               text_source: str,\n",
        "               device: torch.device) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  Вычисляет вектор текстового направления\n",
        "  между таргетом и текущим стилем для множества промптов.\n",
        "\n",
        "  :param models: список моделей, препроцессоров и весов CLIP\n",
        "  :param text_target: текст целевого стиля\n",
        "  :param text_source: текст исходного стиля\n",
        "  :param device: устройство для вычислений (CPU или CUDA)\n",
        "\n",
        "  :return: вектор текстового направления\n",
        "  \"\"\"\n",
        "\n",
        "  prompts_src = make_prompt_list(text_source)\n",
        "  prompts_tgt = make_prompt_list(text_target)\n",
        "\n",
        "  txt_src = preprocessing_text(models, prompts_src, device)\n",
        "  txt_tgt = preprocessing_text(models, prompts_tgt, device)\n",
        "\n",
        "  delta = txt_tgt - txt_src\n",
        "  delta = delta / delta.norm(dim=-1, keepdim=True)\n",
        "\n",
        "  return delta"
      ],
      "metadata": {
        "id": "xNBJyxzCAT3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def delta_image(models: list,\n",
        "                image: torch.Tensor,\n",
        "                image_frozen: torch.Tensor,\n",
        "                device: torch.device) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  Вычисляет вектор визуального направления\n",
        "  между изображением до и после fine-tuning.\n",
        "\n",
        "  :param models: список моделей, препроцессоров и весов CLIP\n",
        "  :param image: изображение из обучаемого генератора\n",
        "  :param image_frozen: изображение из замороженного генератора\n",
        "  :param device: устройство для вычислений (CPU или CUDA)\n",
        "\n",
        "  :return: вектор визуального направления\n",
        "  \"\"\"\n",
        "  img = preprocessing_image(models, image, device)\n",
        "  img_frz = preprocessing_image(models, image_frozen, device)\n",
        "\n",
        "  delta = img - img_frz\n",
        "  delta = delta / delta.norm(dim=-1, keepdim=True)\n",
        "\n",
        "  return delta"
      ],
      "metadata": {
        "id": "6DKjMJjt1Ejw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clip_loss(delta_image: torch.Tensor,\n",
        "              delta_text: torch.Tensor) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  Вычисляет направленный CLIP-loss.\n",
        "\n",
        "  :param delta_image: вектор визуального направления\n",
        "  :param delta_text: вектор текстового направления\n",
        "\n",
        "  :return: направленный CLIP-loss (скаляр)\n",
        "  \"\"\"\n",
        "  direction_clip_loss = 1 - F.cosine_similarity(delta_image, delta_text, dim=-1)\n",
        "  return direction_clip_loss.mean()"
      ],
      "metadata": {
        "id": "CSrnhQ_8DbtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "uRpZMN5H0SKt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### clip_loss"
      ],
      "metadata": {
        "id": "B8d1K1aQVcWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_generator(generator,\n",
        "                    generator_frozen,\n",
        "                    model_names: list[str],\n",
        "                    text_source: str,\n",
        "                    text_target: str,\n",
        "                    epochs: int,\n",
        "                    criterion,\n",
        "                    device: torch.device,\n",
        "                    RAND: int=2,\n",
        "                    batch_size: int=2,\n",
        "                    sample_size: int=4,\n",
        "                    batch_size_w: int=8,\n",
        "                    lr: float=0.002,\n",
        "                    lr_w: float=0.02,\n",
        "                    k: int=12,\n",
        "                    lambda_patch: float=0.1,\n",
        "                    lambda_l2: float=0.001,\n",
        "                    max_norm: float=1.0,\n",
        "                    weights: list[float]=[1.0, 1.0],\n",
        "                    use_conv_layers: bool=False,\n",
        "                    c: torch.Tensor=None):\n",
        "  \"\"\"\n",
        "  Обучает генератор StyleGAN2 для изменения изображений в направлении текста\n",
        "  с использованием CLIP-косинусного расстояния в качестве функции потерь.\n",
        "\n",
        "  Функция выполняет fine-tuning генератора на фиксированном латентном векторе,\n",
        "  вычисляет векторное направление между изображениями до и после обучения\n",
        "  и текстовыми эмбеддингами, и минимизирует косинусное расстояние между ними.\n",
        "\n",
        "  Каждые 50 эпох сохраняет сгенерированные изображения.\n",
        "  В конце визуализирует график лосса.\n",
        "\n",
        "  :param generator: генератор StyleGAN2, который будет обучаться\n",
        "  :param generator_frozen: замороженный генератор, используемый для вычисления delta_image\n",
        "  :param model_names: список имен моделей CLIP\n",
        "  :param text_source: исходный текстовый промпт\n",
        "  :param text_target: целевой текстовый промпт\n",
        "  :param epochs: количество эпох обучения генератора\n",
        "  :param criterion: функция потерь, которая принимает delta_image и delta_text и возвращает скалярный loss\n",
        "  :param device: устройство для вычислений (CPU или CUDA)\n",
        "  :param RAND: фиксирует генератор случайных чисел\n",
        "  :param batch_size: размерность батча для обучения\n",
        "  :param sample_size: размерность генерируемого сэмпла\n",
        "  :param batch_size_w: размерность батча для оптимизации векторов из пространства W+\n",
        "  :param lr: скорость обучения для оптимизатора параметров генератора\n",
        "  :param lr_w: скорость обучения для оптимизатора векторов из пространства W+\n",
        "  :param k: количество слоев которые обучаются\n",
        "  :param lambda_patch: коэффициент патч-регуляризации\n",
        "  :param lambda_l2: коэффициент l2-регуляризации\n",
        "  :param max_norm: максимальная норма градиентов\n",
        "  :param weights: вклад модели CLIP в итоговый эмбеддинг\n",
        "  :param use_conv_layers: флаг для разморозки сверточных слоев генератора\n",
        "  :param c: условие для генератора\n",
        "\n",
        "  :return: None\n",
        "  \"\"\"\n",
        "  loss_lst = []\n",
        "\n",
        "  torch.cuda.empty_cache()\n",
        "  torch.cuda.manual_seed(RAND)\n",
        "  os.makedirs('results', exist_ok=True)\n",
        "\n",
        "  latent_sample = torch.randn(sample_size, generator.z_dim, device=device)\n",
        "\n",
        "  models = load_clip_models(model_names,\n",
        "                            device=device,\n",
        "                            weights=weights)\n",
        "\n",
        "  generator = generator.to(device)\n",
        "  generator_frozen = generator_frozen.to(device)\n",
        "  generator.train()\n",
        "  generator_frozen.eval()\n",
        "\n",
        "  lst_block = []\n",
        "  for name, module in generator.synthesis.named_children():\n",
        "    if name.startswith('b'):\n",
        "        lst_block.append(name)\n",
        "\n",
        "  # Заморозка всего generator_frozen\n",
        "  for param in generator_frozen.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "  # Заморозка mapping generator\n",
        "  for param in generator.mapping.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "  optimizer = torch.optim.Adam(generator.synthesis.parameters(), lr=lr)\n",
        "\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "\n",
        "    # Оптимизация векторов из пространства W+\n",
        "    latent_plus = torch.randn(batch_size_w, generator.z_dim, device=device)\n",
        "    w_plus_source = generator.mapping(latent_plus,\n",
        "                                      c) # [batch_size, num_layers, dim]\n",
        "    w_plus_target = generator.mapping(latent_plus,\n",
        "                                      c) # [batch_size, num_layers, dim]\n",
        "    w_plus_target = w_plus_target.clone().requires_grad_(True)\n",
        "\n",
        "    optimizer_w_plus = torch.optim.Adam([w_plus_target], lr=lr_w)\n",
        "\n",
        "    optimizer_w_plus.zero_grad()\n",
        "\n",
        "    image_w_plus = generator.synthesis(w_plus_target,\n",
        "                                       noise_mode='const')\n",
        "\n",
        "    image_w_plus_features = preprocessing_image(models,\n",
        "                                                image_w_plus,\n",
        "                                                device)\n",
        "\n",
        "    prompts_tgt = make_prompt_list(text_target)\n",
        "\n",
        "    text_w_plus_features = preprocessing_text(models,\n",
        "                                              prompts_tgt,\n",
        "                                              device)\n",
        "\n",
        "    loss_w_plus = cosine_dist(image_w_plus_features,\n",
        "                              text_w_plus_features)\n",
        "\n",
        "    loss_w_plus.backward()\n",
        "\n",
        "    # Ограничивает норму градиентов\n",
        "    torch.nn.utils.clip_grad_norm_([w_plus_target], max_norm)\n",
        "\n",
        "    optimizer_w_plus.step()\n",
        "\n",
        "    delta_w_plus = w_plus_target.detach() - w_plus_source.detach()\n",
        "    delta_norm = delta_w_plus.mean(dim=0).norm(p=2, dim=-1)\n",
        "\n",
        "    topk_indices = torch.topk(delta_norm, k).indices\n",
        "\n",
        "    # Удаляет отработанные тензоры\n",
        "    del latent_plus, w_plus_source, w_plus_target, image_w_plus\n",
        "    del image_w_plus_features, text_w_plus_features, loss_w_plus, delta_w_plus, delta_norm\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Заморозка synthesis\n",
        "    for p in generator.synthesis.parameters():\n",
        "      p.requires_grad = False\n",
        "\n",
        "    # Заморозка affine и torgb параметров\n",
        "    #for name, module in generator.synthesis.named_modules():\n",
        "    #  lname = name.lower()\n",
        "    #  if 'affine' in lname or 'torgb' in lname:\n",
        "    #    for p in module.parameters():\n",
        "    #      p.requires_grad = False\n",
        "\n",
        "    # Заморозка всех слоев кроме топ-k\n",
        "    for ind in topk_indices:\n",
        "      block_idx = ind // 2\n",
        "      conv_num = ind % 2\n",
        "\n",
        "      block_name = lst_block[block_idx]\n",
        "      block = getattr(generator.synthesis, block_name)\n",
        "\n",
        "      if use_conv_layers:\n",
        "      # Заморозка всех слоев кроме топ-k conv слоев\n",
        "        if block_name == 'b4':\n",
        "          block.conv1.weight.requires_grad = True\n",
        "          block.conv1.bias.requires_grad = True\n",
        "        else:\n",
        "          if conv_num == 0:\n",
        "            block.conv0.weight.requires_grad = True\n",
        "            block.conv0.bias.requires_grad = True\n",
        "          else:\n",
        "            block.conv1.weight.requires_grad = True\n",
        "            block.conv1.bias.requires_grad = True\n",
        "      else:\n",
        "        # Заморозка всех слоев кроме топ-k affine и conv слоев\n",
        "        if block_name == 'b4':\n",
        "          block.conv1.weight.requires_grad = True\n",
        "          block.conv1.bias.requires_grad = True\n",
        "          block.conv1.affine.weight.requires_grad = True\n",
        "          block.conv1.affine.bias.requires_grad = True\n",
        "        else:\n",
        "          if conv_num == 0:\n",
        "            block.conv0.weight.requires_grad = True\n",
        "            block.conv0.bias.requires_grad = True\n",
        "            block.conv0.affine.weight.requires_grad = True\n",
        "            block.conv0.affine.bias.requires_grad = True\n",
        "          else:\n",
        "            block.conv1.weight.requires_grad = True\n",
        "            block.conv1.bias.requires_grad = True\n",
        "            block.conv1.affine.weight.requires_grad = True\n",
        "            block.conv1.affine.bias.requires_grad = True\n",
        "\n",
        "    train_params = [param for param in generator.parameters() if param.requires_grad]\n",
        "    # обновляет параметры оптимизатора\n",
        "    optimizer.param_groups[0]['params'] = train_params\n",
        "    #optimizer = torch.optim.Adam(train_params, lr=lr)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    latent = torch.randn(batch_size, generator.z_dim, device=device)\n",
        "    with torch.no_grad():\n",
        "      image_frozen = generator_frozen(latent,\n",
        "                                      c,\n",
        "                                      truncation_psi=1.0,\n",
        "                                      noise_mode='const')\n",
        "\n",
        "    image = generator(latent,\n",
        "                      c,\n",
        "                      truncation_psi=1.0,\n",
        "                      noise_mode='const')\n",
        "\n",
        "    emb_delta_image = delta_image(models,\n",
        "                                  image,\n",
        "                                  image_frozen,\n",
        "                                  device)\n",
        "\n",
        "    emb_delta_text = delta_text(models,\n",
        "                                text_target,\n",
        "                                text_source,\n",
        "                                device)\n",
        "    # l2-регуляризация\n",
        "    reg_loss = 0.0\n",
        "    reg_count = 0\n",
        "    for p_frozen, p in zip(generator_frozen.synthesis.parameters(),\n",
        "                           generator.synthesis.parameters()):\n",
        "      if p.requires_grad:\n",
        "        reg_loss += (p_frozen.detach() - p).pow(2).sum()\n",
        "        reg_count += p.numel()\n",
        "    if reg_count > 0:\n",
        "      reg_loss = reg_loss / (reg_count + 1e-8)\n",
        "    else:\n",
        "      reg_loss = torch.tensor(0.0, device=device)\n",
        "\n",
        "    # Патч-регуляризация\n",
        "    # Извлекаются патчи\n",
        "    patches = extract_patches(image,\n",
        "                              patch_size=64,\n",
        "                              num_patches=8)\n",
        "    patches_frozen = extract_patches(image_frozen,\n",
        "                                     patch_size=64,\n",
        "                                     num_patches=8)\n",
        "\n",
        "    emb_patches = preprocessing_image(models,\n",
        "                                      patches,\n",
        "                                      device)\n",
        "    emb_patches_frozen = preprocessing_image(models,\n",
        "                                             patches_frozen,\n",
        "                                             device)\n",
        "\n",
        "    patch_loss = cosine_dist(emb_patches,\n",
        "                             emb_patches_frozen)\n",
        "\n",
        "    loss = criterion(emb_delta_image,\n",
        "                     emb_delta_text)\n",
        "\n",
        "    loss = loss + lambda_patch * patch_loss + lambda_l2 * reg_loss\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    # Ограничивает норму градиентов\n",
        "    torch.nn.utils.clip_grad_norm_(train_params, max_norm)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_lst.append(loss.item())\n",
        "\n",
        "    print(f'Эпоха {epoch + 1}: loss={loss.item():.4f}')\n",
        "\n",
        "    # Удаляет отработанные тензоры\n",
        "    del latent, image_frozen, image, emb_delta_image, emb_delta_text\n",
        "    del patches, patches_frozen, emb_patches_frozen, loss\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "      image_sample = generator(latent_sample,\n",
        "                               c,\n",
        "                               truncation_psi=0.7,\n",
        "                               noise_mode='const')\n",
        "      img = (image_sample.clamp(-1, 1) + 1) / 2.0  # [-1, 1] → [0, 1]\n",
        "      grid = make_grid(img, nrow=2)\n",
        "\n",
        "      save_path = f'results/generated_image_epoch_{epoch}.jpg'\n",
        "      save_image(grid, save_path, 'jpeg')\n",
        "\n",
        "      plt.figure(figsize=(8, 8))\n",
        "      plt.imshow(grid.permute(1, 2, 0).detach().cpu().numpy())\n",
        "      plt.axis('off')\n",
        "      plt.show()\n",
        "\n",
        "      # Удаляет отработанные тензоры\n",
        "      del image_sample, img\n",
        "\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "  plt.figure(figsize=(8, 8))\n",
        "  plt.plot(loss_lst)\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel('loss')\n",
        "  plt.title('График лосса')\n",
        "  plt.grid(True)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "xNZlBxSMVO1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator(generator=G,\n",
        "                generator_frozen=G_frozen,\n",
        "                model_names=['ViT-B/32', 'ViT-B/16'],\n",
        "                text_source='human',\n",
        "                text_target='werewolf',\n",
        "                epochs=301,\n",
        "                criterion=clip_loss,\n",
        "                device=device,\n",
        "                lr=0.003,\n",
        "                lr_w=0.002,\n",
        "                lambda_patch=0.5,\n",
        "                lambda_l2=0.1,\n",
        "                max_norm=1.0,\n",
        "                weights=[1.0, 1.0],\n",
        "                use_conv_layers=True,\n",
        "                c=None)"
      ],
      "metadata": {
        "id": "gW9u3Sswnzwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y6XIPTepIa8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UIjIIzH5g139"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}